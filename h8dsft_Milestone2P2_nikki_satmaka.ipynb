{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF_1YtlqvCys"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "Nikki Satmaka - Batch 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OlWh7mnvCyw"
      },
      "source": [
        "## Description\n",
        "\n",
        "Dataset is taken from [Kaggle](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset)\n",
        "\n",
        "Context:\n",
        "\n",
        "This dataset contains \n",
        "\n",
        "### Objective\n",
        "\n",
        "- pass\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "- pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqhEvvettBzg"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WFQkTo0bvCP"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9APP_mpGaiCo"
      },
      "outputs": [],
      "source": [
        "# prepare kaggle environment\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/03-resources/kaggle/kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci3jQchR2QOS"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "!mkdir data\n",
        "!kaggle datasets download --p data --unzip clmentbisaillon/fake-and-real-news-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1RmPJxvdoGL"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install transformers\n",
        "!pip install feature-engine --quiet --progress-bar off\n",
        "!python -m spacy download en_core_web_sm --quiet --progress-bar off\n",
        "!python -m nltk.downloader stopwords punkt --quiet \n",
        "\n",
        "# copy packages directory\n",
        "!cp -r /content/drive/MyDrive/03-resources/python_pkgs/packages ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGE5mtqbvCyy"
      },
      "source": [
        "# 2. Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9s0COjpvCyz"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Split Dataset and Standarize the Datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# Neural Network\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding\n",
        "from tensorflow.keras.layers import InputLayer, Dense\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM, GRU\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "# Evaluate Classification Models\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Useful functions\n",
        "from packages.checker import check_missing\n",
        "from packages.checker import check_links_only\n",
        "from packages.outlier_handling import outlier_summary\n",
        "from packages.visualization import kdeplot, plot_loss, plot_acc\n",
        "from packages.imputation_handling import drop_title_links_only\n",
        "\n",
        "from packages.text_preprocessing import combine_text\n",
        "from packages.text_preprocessing import clean_text, strip_stopwords \n",
        "from packages.text_preprocessing import stem_text, lemmatize_text\n",
        "\n",
        "\n",
        "pd.set_option('display.precision', 2)\n",
        "\n",
        "sns.set_theme(style='darkgrid', palette='Set1')\n",
        "\n",
        "# set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLHhDyPTvCy1"
      },
      "source": [
        "# 3. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLRFZhvbsfmW"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "df_fake_ori = pd.read_csv(\"data/Fake.csv\")\n",
        "df_real_ori = pd.read_csv(\"data/True.csv\")\n",
        "\n",
        "# make a copy of the original dataframe\n",
        "df_fake = df_fake_ori.copy()\n",
        "df_real = df_real_ori.copy()\n",
        "\n",
        "# display the first 5 entries of fake news data\n",
        "df_fake.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Jlwf2h6sfmX"
      },
      "outputs": [],
      "source": [
        "# display the first 5 entries of real news data\n",
        "df_real.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CBOll5QvCy3"
      },
      "source": [
        "## Data Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miBylZ1uvCy4"
      },
      "outputs": [],
      "source": [
        "# check dataset shape\n",
        "print(f\"Fake news dataset shape: {df_fake.shape}\")\n",
        "print(f\"Real news dataset shape: {df_real.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z86Ksav6vCy5"
      },
      "source": [
        "There are 21417 instances and 4 columns of real news data\\\n",
        "There are 23481 instances and 4 columns of fake news data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXsljT5-sfmZ"
      },
      "outputs": [],
      "source": [
        "# check fake news dataset info\n",
        "df_fake.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IitCNUL_vCy5"
      },
      "outputs": [],
      "source": [
        "# check real news dataset info\n",
        "df_real.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tgjo-Qcsfma"
      },
      "source": [
        "Both dataset have their date as string object. I'm going to convert them to datetime object.\\\n",
        "However, I'm going to combine them first. Both dataframes have the same features, so we can safely proceed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8dR1rX_sfma"
      },
      "source": [
        "## Combine Dataset\n",
        "Since the dataset was separated between real and fake news, let's combine them into one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Imky-dGsfma"
      },
      "outputs": [],
      "source": [
        "# define label, 0 for fake news, 1 for real news\n",
        "df_fake['label'] = 0\n",
        "df_real['label'] = 1\n",
        "\n",
        "# concat datasets and reset index\n",
        "df_ori = pd.concat([df_fake, df_real]).reset_index(drop=True)\n",
        "\n",
        "# create backup\n",
        "df = df_ori.copy()\n",
        "\n",
        "# display the first five rows of the dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhLPUlM-sfmb"
      },
      "outputs": [],
      "source": [
        "# check dataset shape\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUfJWNJzsfmb"
      },
      "source": [
        "There are 44,898 instances of data with 5 columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exBJ6mQs8ePn"
      },
      "source": [
        "## Check Missing values and Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxJgvS9Isfmb"
      },
      "outputs": [],
      "source": [
        "# check missing values in dataset\n",
        "check_missing(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVoBOGPssfmc"
      },
      "source": [
        "Great! There are no missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y92F6_xvCy7"
      },
      "outputs": [],
      "source": [
        "# check duplicate values in dataset\n",
        "df[df.duplicated()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx0j2j9Ysfmc"
      },
      "source": [
        "We found some duplicates in our dataset. Let's drop them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tjwq3omhsfmc"
      },
      "outputs": [],
      "source": [
        "# drop duplicates\n",
        "df = df[~df.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZHlzB3csfmd"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxkpE12Esfmd"
      },
      "source": [
        "We now have 44,689 instances of data left"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3ViTs0XvCy_"
      },
      "source": [
        "## Check for Dataset Imbalance\n",
        "\n",
        "Check whether the label of the dataset is balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC24fl7KvCy_"
      },
      "outputs": [],
      "source": [
        "# check for imbalance in label\n",
        "plt.figure(figsize=(4,5))\n",
        "sns.countplot(data=df, x='label')\n",
        "plt.title('Number of Fake VS Real News')\n",
        "plt.xlabel(None)\n",
        "plt.ylabel(None)\n",
        "plt.ylim(0, df.shape[0] / 1.5)\n",
        "plt.xticks([0, 1], ['Fake', 'Real'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZSmAZZpvCy_"
      },
      "source": [
        "We can see that our data has similar number of fake and real news. Hence, the data is quite balanced and we won't need to oversample or undersample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47_3At7svCy_"
      },
      "source": [
        "## Splitting Dataset\n",
        "\n",
        "We need to split the dataset into train and test sets before we do any EDA.\\\n",
        "We do our EDA on the train set so as to not have any bias towards the whole dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdrk8f4uvCy_"
      },
      "source": [
        "### Split train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3QQeQY6vCzA"
      },
      "outputs": [],
      "source": [
        "# split sets to training+validation and testing sets\n",
        "df_train_valid, df_test = train_test_split(\n",
        "    df,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "print('df_train_valid Size:', df_train_valid.shape)\n",
        "print('df_test Size:', df_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlJiEd5vvCzA"
      },
      "source": [
        "### Split train and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0A67hTRvCzA"
      },
      "outputs": [],
      "source": [
        "# split sets to training and validation sets\n",
        "df_train, df_valid = train_test_split(\n",
        "    df_train_valid,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=df_train_valid['label']\n",
        ")\n",
        "\n",
        "print('df_train Size:', df_train.shape)\n",
        "print('df_valid Size:', df_valid.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0uuIdgVsfme"
      },
      "outputs": [],
      "source": [
        "# print datasets shape\n",
        "print(f'df_train shape: {df_train.shape}')\n",
        "print(f'df_valid shape: {df_valid.shape}')\n",
        "print(f'df_test shape: {df_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S95xMVFTvCzB"
      },
      "outputs": [],
      "source": [
        "# backup the train set that we are gonna perform EDA on\n",
        "df_train_ori = df_train.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qk9-XqpvCzB"
      },
      "source": [
        "# 4. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNAYYyR5sfmf"
      },
      "source": [
        "## Subjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoFWBDptsfmf"
      },
      "outputs": [],
      "source": [
        "# plot number of news according to subjects\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.countplot(\n",
        "    data=df_train,\n",
        "    x='subject',\n",
        "    hue='label',\n",
        "    order=df_train['subject'].value_counts().index,\n",
        ")\n",
        "plt.title(f'No. of news according to subjects grouped by fake status')\n",
        "plt.xlabel(None)\n",
        "plt.ylabel('No. of news')\n",
        "\n",
        "plt.legend(labels=['Fake', 'Real'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wPZDOiYsfmf"
      },
      "source": [
        "We can see that real and fake news have totally different subjects. This might be a giveaway if we were to include this feature in our machine learning model later on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4aPbrXM84mL"
      },
      "source": [
        "## Date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FhAG8Oj85_o"
      },
      "outputs": [],
      "source": [
        "# attempt to convert date to datetime object to analyze time intervals\n",
        "try:\n",
        "    pd.to_datetime(df_train['date'])\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-Y3Rzyq92xn"
      },
      "source": [
        "That's weird. Why would there be a string in a date feature? Let alone a link.\\\n",
        "Let's check for any links in the date feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI3QkxeN-Dsn"
      },
      "outputs": [],
      "source": [
        "# check for dates which contains link\n",
        "df_train[df_train['date'].str.contains('http')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJrzbTVq-QiR"
      },
      "source": [
        "There are 6 of them, and it's not just the date feature. The same links can also be found in the title and text features. This could be treated as missing values since they don't contain actually news"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODnukxjIsfmf"
      },
      "source": [
        "## Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H6HLK0Ssfmg"
      },
      "outputs": [],
      "source": [
        "# display the first instance of the dataset\n",
        "df_train.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9BZARd2sfmg"
      },
      "outputs": [],
      "source": [
        "# display the title of the first instance \n",
        "df_train.iloc[0]['title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L7VHFcusfmg"
      },
      "outputs": [],
      "source": [
        "# display the title of the first text \n",
        "df_train.iloc[0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJTiTvphsfmg"
      },
      "source": [
        "It seems like `title` and `text` are two features which make the most important content of the news. Since NLP are performed on a body of text, I'm only going to use one feature, so I'm going to combine these features into one later on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tRDYm4iBLNb"
      },
      "source": [
        "## Wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CYaae9iEe4s"
      },
      "outputs": [],
      "source": [
        "# create word cloud object\n",
        "wc =  WordCloud(\n",
        "    max_words=2000,\n",
        "    stopwords=STOPWORDS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2viPoiUQBOue"
      },
      "outputs": [],
      "source": [
        "# generate word cloud\n",
        "wc.generate(' '.join(df_train['text']))\n",
        "\n",
        "# plot wordcloud\n",
        "plt.figure(figsize=(18, 12))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0ZcfFhCEBsU"
      },
      "source": [
        "We can see that `Donald Trump` and `White House` show up quite often.\\\n",
        "Let's separate between the fake and real news and see how they differ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NiGYKS6Eigt"
      },
      "outputs": [],
      "source": [
        "# generate word cloud for fake news\n",
        "wc.generate(' '.join(df_train[df_train['label'] == 0]['text']))\n",
        "\n",
        "# plot wordcloud\n",
        "plt.figure(figsize=(18, 12))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBU7F0dkFAw6"
      },
      "source": [
        "For fake news, there's and additional word, `said` that popped out. This is interesting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUsnr5pdEqbh"
      },
      "outputs": [],
      "source": [
        "# generate word cloud\n",
        "wc.generate(' '.join(df_train[df_train['label'] == 1]['text']))\n",
        "\n",
        "# plot wordcloud\n",
        "plt.figure(figsize=(18, 12))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWyg4bu9EZWB"
      },
      "source": [
        "For real news, the words are pretty similar, like `said`, `United State`, and also `Donald Trump`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMS5D1nLsNnc"
      },
      "source": [
        "## Number of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2FuFhmCsNnc"
      },
      "outputs": [],
      "source": [
        "# create a feature to store the number of words in the text feature\n",
        "df_train['words'] = df_train['text'].str.split().apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILsOyBwksNnc"
      },
      "outputs": [],
      "source": [
        "# plot number of words\n",
        "plt.figure(figsize=(6, 10))\n",
        "sns.boxplot(data=df_train, x='label', y='words', showfliers=False)\n",
        "plt.title(f'Words per News')\n",
        "plt.xlabel(None)\n",
        "plt.ylabel('No. of words')\n",
        "plt.xticks([0, 1], ['Fake', 'Real'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JoeZsj4sNnf"
      },
      "source": [
        "We can see that both fake and real news have about 400 words in their body of text. However, the first quartile for real news is lower than that of fake news\\\n",
        "However, we had the outliers hidden for that plot, now let's show them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac-NjQ66sNng"
      },
      "outputs": [],
      "source": [
        "# plot number of words\n",
        "plt.figure(figsize=(6, 10))\n",
        "sns.boxplot(data=df_train, x='label', y='words', showfliers=True)\n",
        "plt.title(f'Words per News')\n",
        "plt.xlabel(None)\n",
        "plt.ylabel('No. of words')\n",
        "plt.xticks([0, 1], ['Fake', 'Real'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5QBVR_AsNng"
      },
      "source": [
        "It's now a totally different story. We can see that there are a couple fake news which have a lot more words compared to real news"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxsuqxtlsNnh"
      },
      "source": [
        "## Unique Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c3HPC5KsNnh"
      },
      "outputs": [],
      "source": [
        "# create a feature to store the number of words in the text feature\n",
        "df_train['unique_words'] = df_train['text'].str.split().apply(np.unique).apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1SOl0XTsNni"
      },
      "outputs": [],
      "source": [
        "# plot number of unique words\n",
        "plt.figure(figsize=(6, 8))\n",
        "sns.barplot(data=df_train, x='label', y='unique_words', ci=None)\n",
        "plt.title(f'Unique Words per News')\n",
        "plt.xlabel(None)\n",
        "plt.ylabel('No. of words')\n",
        "plt.xticks([0, 1], ['Fake', 'Real'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEFFe0irsNni"
      },
      "source": [
        "The number of unique words in fake news is slightly higher than in real news"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEaDBEhyvCzM"
      },
      "source": [
        "# 5. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khPsaq1uvCzM"
      },
      "outputs": [],
      "source": [
        "# restore the train set from the backup\n",
        "df_train = df_train_ori.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZZ0G_odvCzM"
      },
      "outputs": [],
      "source": [
        "# split between features and label\n",
        "X_train = df_train.drop(['label'], axis=1)\n",
        "y_train = df_train['label'].copy()\n",
        "\n",
        "X_valid = df_valid.drop(['label'], axis=1)\n",
        "y_valid = df_valid['label'].copy()\n",
        "\n",
        "X_test = df_test.drop(['label'], axis=1)\n",
        "y_test = df_test['label'].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8IoAq7svCzN"
      },
      "source": [
        "## Handling Missing Values\n",
        "\n",
        "We don't have any nan missing values. However, we found out during our EDA that some instances in our dataset contains nothing but links. Therefore, we are going to drop these entries as they do not provide any value in "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiOZvEGuCOR8"
      },
      "outputs": [],
      "source": [
        "# check links only value in train set\n",
        "check_links_only(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4r2Gk1KMZEq"
      },
      "outputs": [],
      "source": [
        "# check links only value in validation set\n",
        "check_links_only(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgnMRuq_MbrN"
      },
      "outputs": [],
      "source": [
        "# check links only value in test set\n",
        "check_links_only(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xFnzoC4vCzO"
      },
      "source": [
        "It seems like there are some instances of data whose title is not link, but the text contains only link. Let's check these data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8qE_BBqNJcJ"
      },
      "outputs": [],
      "source": [
        "# display the first five rows of data which have normal titles, but link in text\n",
        "X_train[\n",
        "    ~(X_train['title'].str.contains(r'^http\\S+$', regex=True)) &\n",
        "    (X_train['text'].str.contains(r'^http\\S+$', regex=True))\n",
        "].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dANNfi3ZOoVT"
      },
      "source": [
        "It seems like the links in the text point out to different websites.\\\n",
        "I'm not going to drop these kinds of instances, since the values in the title feature could still be useful as predictors.\\\n",
        "Therefore, I'm only dropping instances of data which title is links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7CjDWO37NZJ"
      },
      "outputs": [],
      "source": [
        "# list of features that we want to impute\n",
        "impute_cols = ['title', 'date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7FHOkJLPDEc"
      },
      "outputs": [],
      "source": [
        "# print dataset shape before handling links in title\n",
        "print('X_train and y_train shape before handling links in title:', X_train.shape, y_train.shape)\n",
        "print('X_valid and y_valid shape before handling links in title:', X_valid.shape, y_valid.shape)\n",
        "print('X_test and y_test shape before handling links in title:', X_test.shape, y_test.shape)\n",
        "\n",
        "print('=' * 80)\n",
        "\n",
        "# drop instances of data which has link as its title\n",
        "X_train, y_train_final = drop_title_links_only(X_train, y_train, impute_cols)\n",
        "X_valid, y_valid_final = drop_title_links_only(X_valid, y_valid, impute_cols)\n",
        "X_test, y_test_final = drop_title_links_only(X_test, y_test, impute_cols)\n",
        "\n",
        "# print dataset shape after handling links in title\n",
        "print('X_train and y_train_final shape after handling links in title:', X_train.shape, y_train_final.shape)\n",
        "print('X_valid and y_valid_final shape after handling links in title:', X_valid.shape, y_valid_final.shape)\n",
        "print('X_test and y_test_final shape after handling links in title:', X_test.shape, y_test_final.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD2pj-3lvCzR"
      },
      "source": [
        "Great! We have no more missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIb_3v0YznHU"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TJG5W-gz-2E"
      },
      "outputs": [],
      "source": [
        "# display the first five rows of the train set\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxPimvxz0U_W"
      },
      "source": [
        "- We mentioned that we are going to combine the `title` and `text` features since they both make up the major part of a news\n",
        "- We're going to drop `subject`, since we found out during our EDA that fake and real news have totally different subjects\n",
        "- We're also going to drop `date`, since date do not have any influence in an NLP model. We're not attempting to find a pattern on when a fake news might be released. We're attempting to spot a fake news based on its content\n",
        "- Therefore, our dataset will only contain the feature which contains news from `title` and `text`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQAO-jz7sfmg"
      },
      "outputs": [],
      "source": [
        "# combine title and text features as news\n",
        "X_train_combined = combine_text(X_train, 'news', 'title', 'text')\n",
        "X_valid_combined = combine_text(X_valid, 'news', 'title', 'text')\n",
        "X_test_combined = combine_text(X_test, 'news', 'title', 'text')\n",
        "\n",
        "# keep only the news feature as the only text to process\n",
        "X_train_combined = X_train_combined['news']\n",
        "X_valid_combined = X_valid_combined['news']\n",
        "X_test_combined = X_test_combined['news']\n",
        "\n",
        "# print datasets shape\n",
        "print(f'X_train_combined shape: {X_train_combined.shape}')\n",
        "print(f'X_valid_combined shape: {X_valid_combined.shape}')\n",
        "print(f'X_test_combined shape: {X_test_combined.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttnR-hVU3UXP"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Noz4A05IzpcZ"
      },
      "outputs": [],
      "source": [
        "# display the first five rows of the train set\n",
        "X_train_combined.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAui_h3l7NZN"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# clean text\n",
        "X_train_cleaned = X_train_combined.apply(clean_text)\n",
        "X_valid_cleaned = X_valid_combined.apply(clean_text)\n",
        "X_test_cleaned = X_test_combined.apply(clean_text)\n",
        "\n",
        "# lemmatize text\n",
        "X_train_lemmatized = X_train_cleaned.apply(lemmatize_text)\n",
        "X_valid_lemmatized = X_valid_cleaned.apply(lemmatize_text)\n",
        "X_test_lemmatized = X_test_cleaned.apply(lemmatize_text)\n",
        "\n",
        "# # stem text\n",
        "# X_train_stemmed = X_train_lemmatized.apply(stem_text)\n",
        "# X_valid_stemmed = X_valid_lemmatized.apply(stem_text)\n",
        "# X_test_stemmed = X_test_lemmatized.apply(stem_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL2Uu6Rd1Jqc"
      },
      "source": [
        "## Create TensorFlow input pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycz2vXd4-bGW"
      },
      "outputs": [],
      "source": [
        "# define final dataset\n",
        "X_train_final = X_train_lemmatized\n",
        "X_valid_final = X_valid_lemmatized\n",
        "X_test_final = X_test_lemmatized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nq-kW4lKpc_"
      },
      "outputs": [],
      "source": [
        "# define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# create tf dataset instance \n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_final, y_train_final)).batch(batch_size).cache()\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid_final, y_valid_final)).batch(batch_size).cache()\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_final, y_test_final)).batch(batch_size).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtPcn0VC8SN4"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaaMW_cZ8Wmw"
      },
      "outputs": [],
      "source": [
        "# declare vectorizer object\n",
        "Vectorize = CountVectorizer()\n",
        "\n",
        "# fit vectorize object to the train set\n",
        "Vectorize.fit(X_train_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GUyPoc9_QaX"
      },
      "outputs": [],
      "source": [
        "# display the top ten vocabs\n",
        "print(f'There are {len(Vectorize.vocabulary_.keys())} vocabulary')\n",
        "print('These are the top ten:')\n",
        "print(list(Vectorize.vocabulary_.keys())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsnYp-bD-7ul"
      },
      "outputs": [],
      "source": [
        "# define percentage of vocab to include\n",
        "pct_vocab = 0.75\n",
        "\n",
        "# define max number of features to include\n",
        "max_features = int(np.floor(0.75 * len(Vectorize.vocabulary_.keys())))\n",
        "\n",
        "# define max number of sequence length based on mean sentence length\n",
        "max_seq_length = int(np.floor(np.mean([len(i.split(' ')) for i in X_train_final])))\n",
        "\n",
        "# print max number of vocabs and output sequence length\n",
        "print(f'Max number of vocabs: {max_features}')\n",
        "print(f'Output sequence length: {max_seq_length}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuREnRA_QQT7"
      },
      "source": [
        "## TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBoxY_Q5Jnm4"
      },
      "outputs": [],
      "source": [
        "# define text vectorizer layer\n",
        "text_vectorizer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    split='whitespace',\n",
        "    ngrams=None,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=max_seq_length,\n",
        ")\n",
        "\n",
        "# adapt vectorization layer to the train set\n",
        "text_vectorizer.adapt(X_train_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF8blHSoQUsc"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfFE1TPIR5eU"
      },
      "outputs": [],
      "source": [
        "# define embbeding layer\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=max_features,\n",
        "    output_dim=128,\n",
        "    input_length=max_seq_length,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inJHRQ9OvCzW"
      },
      "source": [
        "# 6. Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaEdSlDXvCzW"
      },
      "source": [
        "- Target: Predicting whether the client would stop their subscription and leave the company\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "160Pz3xkvCzX"
      },
      "source": [
        "- Predictors: The features I'm going to use are\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7oJGpMavCzX"
      },
      "source": [
        "- Models: I'm going to use "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aL5zwZ5vCzX"
      },
      "source": [
        "## DNN Model\n",
        "\n",
        "Running this model as minimal as possible. Using 1 hidden layer with 8 neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02dgirnOsNn0"
      },
      "outputs": [],
      "source": [
        "# instantiate input object\n",
        "inputs = Input(shape=(1,), dtype=tf.string)\n",
        "\n",
        "# preprocess inputs\n",
        "preprocessed_inputs = text_vectorizer(inputs)\n",
        "\n",
        "# apply model layers\n",
        "x = embedding_layer(preprocessed_inputs)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "x = Dense(units=64, activation='relu')(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "outputs = Dense(units=1, activation='sigmoid')(x)\n",
        "model_dnn = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# compile model\n",
        "model_dnn.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# display model summary for functional model\n",
        "model_dnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-jKAhBgvCzY"
      },
      "outputs": [],
      "source": [
        "# plot model architecture\n",
        "keras.utils.plot_model(model_dnn, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX6pl2uMvCzZ"
      },
      "source": [
        "## LSTM Model\n",
        "\n",
        "Attempting to improve the sequential model using initializer, regularizer, and also dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3p0IGM1vCzZ"
      },
      "outputs": [],
      "source": [
        "# instantiate input object\n",
        "inputs = Input(shape=(1,), dtype=tf.string)\n",
        "\n",
        "# preprocess inputs\n",
        "preprocessed_inputs = text_vectorizer(inputs)\n",
        "\n",
        "# apply model layers\n",
        "x = embedding_layer(preprocessed_inputs)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "x = LSTM(units=64, activation='tanh')(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "outputs = Dense(units=1, activation='sigmoid')(x)\n",
        "model_lstm = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# compile model\n",
        "model_lstm.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# display model summary for functional model\n",
        "model_lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51HfL6mHvCzZ"
      },
      "outputs": [],
      "source": [
        "# plot model architecture\n",
        "keras.utils.plot_model(model_lstm, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKOFbJpLsNn2"
      },
      "source": [
        "## GRU Model\n",
        "\n",
        "Attempting to improve the sequential model using initializer, regularizer, and also dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQFeL_cksNn2"
      },
      "outputs": [],
      "source": [
        "# instantiate input object\n",
        "inputs = Input(shape=(1,), dtype=tf.string)\n",
        "\n",
        "# preprocess inputs\n",
        "preprocessed_inputs = text_vectorizer(inputs)\n",
        "\n",
        "# apply model layers\n",
        "x = embedding_layer(preprocessed_inputs)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "x = GRU(units=64, activation='tanh')(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "outputs = Dense(units=1, activation='sigmoid')(x)\n",
        "model_gru = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# compile model\n",
        "model_gru.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# display model summary for functional model\n",
        "model_gru.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjhyN9YTsNn2"
      },
      "outputs": [],
      "source": [
        "# plot model architecture\n",
        "keras.utils.plot_model(model_gru, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq2OEwSDvCza"
      },
      "source": [
        "## Callbacks objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcOs63eq33qa"
      },
      "outputs": [],
      "source": [
        "# define callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath='models/checkpoint', monitor='val_loss', save_best_only=True, verbose=0)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14e34AS-vCza"
      },
      "source": [
        "# 7. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZWhsq2CvCza"
      },
      "outputs": [],
      "source": [
        "# create dictionary of models\n",
        "models = {\n",
        "    'dnn': model_dnn,\n",
        "    'lstm': model_lstm,\n",
        "    'gru': model_gru,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68HFo2TavCzb"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# create dictionary to store metrics\n",
        "metrics = {}\n",
        "\n",
        "# loop through models and train\n",
        "for name, model in models.items():\n",
        "    # train model\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=30,\n",
        "        validation_data=valid_dataset,\n",
        "        callbacks=callbacks,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # store metrics\n",
        "    metrics[name] = pd.DataFrame(history.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amrx5KpYvCzb"
      },
      "source": [
        "# 8. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF824otUvCzb"
      },
      "outputs": [],
      "source": [
        "# create dictionary to store evaluation metrics\n",
        "eval_metrics = {\n",
        "    'dnn': {},\n",
        "    'lstm': {},\n",
        "    'gru': {},\n",
        "}\n",
        "\n",
        "# loop through models and evaluate them\n",
        "for name, model in models.items():\n",
        "    # evaluate model\n",
        "    eval_metrics[name]['loss'], eval_metrics[name]['accuracy'] = model.evaluate(\n",
        "        test_dataset,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "# create dataframe from evaluation metrics\n",
        "eval_metrics_df = pd.DataFrame(eval_metrics).T\n",
        "\n",
        "# display evaluation metrics\n",
        "eval_metrics_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkNMYPySvCzb"
      },
      "source": [
        "We can see that the loss actually increased after being tuned. The accuracy also slightly decreased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYJVbzK_vCzb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# plot the loss curves\n",
        "for i, (name, metric) in enumerate(metrics.items()):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plot_loss(metric)\n",
        "    plt.title(f'Training and validation loss for {name}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grKPxu6kvCzc"
      },
      "source": [
        "- Even though the sequential and functional model uses the same hyperparameter, the results are actually slightly different\n",
        "- We can see from the graph, that the functional model is a bit overfitted as we started to see the gap widening the higher the epoch.\n",
        "- The sequential model also had a widening of the gap, albeit more subtly.\n",
        "- The tuned models, be it sequential or functional are now a much more better fit, we could even call it a good fit. We also have the validation loss to be slightly lower than the training loss.\n",
        "- We do have to note though that the absolute value of the loss increased after tuning. So we still have to see how it performs when we use it to predict the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlQrhJcOvCzc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# plot the accuracy curves\n",
        "for i, (name, metric) in enumerate(metrics.items()):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plot_acc(metric)\n",
        "    plt.title(f'Training and validation accuracy for {name}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwOj50wvvCzc"
      },
      "source": [
        "- We can see that the accuracy also somewhat decreased after tuning\n",
        "- The validation accuracy for the tuned models are quite high from the start but didn't increase much as it stabilized later on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1ESHUrrvCzc"
      },
      "source": [
        "## Prepare Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp2Aa4RNvCzc"
      },
      "outputs": [],
      "source": [
        "# set threshold for prediction\n",
        "threshold = 0.5\n",
        "\n",
        "# create dictionary to store predictions:\n",
        "predictions = {\n",
        "    'dnn': {},\n",
        "    'lstm': {},\n",
        "    'gru': {},\n",
        "}\n",
        "\n",
        "# loop through models and make predictions\n",
        "for name, model in models.items():\n",
        "    # make predictions for training set\n",
        "    pred_train = model.predict(X_train_final).reshape(-1)\n",
        "    pred_train = np.where(pred_train > threshold, 1, 0)\n",
        "\n",
        "    # make predictions for test set\n",
        "    pred_test = model.predict(X_test_final).reshape(-1)\n",
        "    pred_test = np.where(pred_test > threshold, 1, 0)\n",
        "\n",
        "    # store predictions in dictionary\n",
        "    predictions[name]['train'] = pred_train\n",
        "    predictions[name]['test'] = pred_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nd_UeAqvCzd"
      },
      "outputs": [],
      "source": [
        "# prepare target names for classification report\n",
        "target_names = [\"Fake\", \"Real\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKLrCQruvCzd"
      },
      "source": [
        "## DNN Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLBAkR1PvCzd"
      },
      "outputs": [],
      "source": [
        "# print classification report for dnn model\n",
        "for name, preds in predictions.items():\n",
        "    if 'dnn' not in name:\n",
        "        continue\n",
        "    for dataset, pred in preds.items():\n",
        "        if dataset == 'train':\n",
        "            print(f'{name} classification report for training set:')\n",
        "            print(classification_report(y_train_final, pred, target_names=target_names))\n",
        "        if dataset == 'test':\n",
        "            print(f'{name} classification report for testing set:')\n",
        "            print(classification_report(y_test_final, pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLFu3nP9vCzd"
      },
      "source": [
        "- We can now see clearly that both model are not actually a very good fit. I don't think it can be called overfit either though. So I'd call it a decent fit, but it sure could be improved, as there is a `0.04` gap between training accuracy and testing accuracy\n",
        "- We can also see that tuning the model successfully increased the recall score. This is important to us as we want to minimize **False Negatives**, since we need to detect potential of churning as much as possible.\n",
        "- The tuned recall score is `0.79` which is quite decent\n",
        "- Therefore, our model could be run on inference, but we should also strive to improve this model further"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjVy6DcivCzd"
      },
      "source": [
        "## LSTM Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB7VwALUvCzd"
      },
      "outputs": [],
      "source": [
        "# print classification report for lstm model\n",
        "for name, preds in predictions.items():\n",
        "    if 'lstm' not in name:\n",
        "        continue\n",
        "    for dataset, pred in preds.items():\n",
        "        if dataset == 'train':\n",
        "            print(f'{name} classification report for training set:')\n",
        "            print(classification_report(y_train_final, pred, target_names=target_names))\n",
        "        if dataset == 'test':\n",
        "            print(f'{name} classification report for testing set:')\n",
        "            print(classification_report(y_test_final, pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYnpkAlYvCzd"
      },
      "source": [
        "- The fit situation is quite similar with the sequential models. However, the fit in the tuned model actually became even wider, with a `0.07` gap between training accuracy and testing accuracy\n",
        "- Tuning the model increased the recall score substantially to `0.82`, which is good\n",
        "- So this model is also good, but I'm quite concerned with the wider gap indicating overfit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp1IXOzVg5gH"
      },
      "source": [
        "## GRU Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMIbmvqtg5gI"
      },
      "outputs": [],
      "source": [
        "# print classification report for gru model\n",
        "for name, preds in predictions.items():\n",
        "    if 'gru' not in name:\n",
        "        continue\n",
        "    for dataset, pred in preds.items():\n",
        "        if dataset == 'train':\n",
        "            print(f'{name} classification report for training set:')\n",
        "            print(classification_report(y_train_final, pred, target_names=target_names))\n",
        "        if dataset == 'test':\n",
        "            print(f'{name} classification report for testing set:')\n",
        "            print(classification_report(y_test_final, pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZeQzW4Rg5gJ"
      },
      "source": [
        "- pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QodE-g1vCze"
      },
      "source": [
        "## Analysis\n",
        "- Tuning successfully increased the recall score\n",
        "- The fit on the functional_tuned model actually became wider, indicating that the fit became a tad worse\n",
        "- Therefore, considering the fit and also the recall score, I'm choosing the **sequential_tuned** model as the best model to be saved and run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol4g2-sivCze"
      },
      "source": [
        "## Save The Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlNztkh6vCze"
      },
      "outputs": [],
      "source": [
        "# prepare directory for saving model\n",
        "model_dir = 'models'\n",
        "model_name = 'nlp_model'\n",
        "\n",
        "# create directory if it does not exist\n",
        "Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model_path = Path(model_dir, model_name)\n",
        "\n",
        "# save model\n",
        "model_lstm.save(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2izYvXRxvCze"
      },
      "source": [
        "# 9. Model Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWLyF80XvCze"
      },
      "source": [
        "## Load The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBAv8T-rvCze"
      },
      "outputs": [],
      "source": [
        "# model location\n",
        "model_dir = 'models'\n",
        "model_name = 'nlp_model'\n",
        "\n",
        "# create path object\n",
        "model_path = Path(model_dir, model_name)\n",
        "\n",
        "# load model\n",
        "model = keras.models.load_model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3HYc5DGvCzf"
      },
      "source": [
        "## Prepare Data For Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oxxriF5vCzf"
      },
      "outputs": [],
      "source": [
        "# prepare data for inferencing\n",
        "\n",
        "# create dataframe for inferencing\n",
        "new_data = pd.DataFrame(new_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-iWElxrvCzf"
      },
      "outputs": [],
      "source": [
        "# display dataframe for inferencing\n",
        "new_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPdLE1WSvCzg"
      },
      "source": [
        "## Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGBpR-5ivCzg"
      },
      "outputs": [],
      "source": [
        "# impute missing values\n",
        "new_data_prepared = impute_total_charges(new_data)\n",
        "\n",
        "# impute no phone service and no internet service with no\n",
        "new_data_prepared = impute_no_phone_internet(new_data_prepared)\n",
        "\n",
        "# print shape of prepared data\n",
        "print(new_data_prepared.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0ahcueXvCzk"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# set threshold for prediction\n",
        "threshold = 0.5\n",
        "\n",
        "# scale inference set\n",
        "new_data_scaled = scaler.transform(new_data_prepared)\n",
        "\n",
        "# encode inference set\n",
        "new_data_encoded = encoder.transform(new_data_scaled)\n",
        "\n",
        "# cast as float32\n",
        "new_data_final = new_data_encoded.astype(np.float32)\n",
        "\n",
        "# predict inference set using the final model\n",
        "y_pred_new = model.predict(new_data_final).reshape(-1)\n",
        "y_pred_new = np.where(y_pred_new > threshold, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9Ibh9x9vCzl"
      },
      "outputs": [],
      "source": [
        "# create dataframe with predictions\n",
        "new_data['pred'] = y_pred_new\n",
        "\n",
        "# display inference set\n",
        "new_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1i5JOLsvCzl"
      },
      "source": [
        "Model successfully run on inference dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdqiSd0ZvCzl"
      },
      "source": [
        "# 10. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxhrLBZFvCzl"
      },
      "source": [
        "## On EDA\n",
        "- pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFNrjLbjvCzm"
      },
      "source": [
        "## On Modeling\n",
        "- pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiN7n8BQvCzm"
      },
      "source": [
        "## Implication\n",
        "- pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V6UtaJtvCzm"
      },
      "source": [
        "## Future Improvement\n",
        "- pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9OlWh7mnvCyw",
        "vxhrLBZFvCzl",
        "JFNrjLbjvCzm",
        "QiN7n8BQvCzm",
        "-V6UtaJtvCzm"
      ],
      "name": "h8dsft_Milestone2P2_nikki_satmaka.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "57744ed932a5da4ffb7d5879d9b65170d321805180660db910f737e8fd70cf58"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 (conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
